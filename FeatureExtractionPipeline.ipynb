{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-UrjNWGIwQM"
      },
      "outputs": [],
      "source": [
        "!pip install -qq PyPDF2\n",
        "import PyPDF2\n",
        "import re\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download(\"words\")\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"words\")\n",
        "\n",
        "swords = stopwords.words('english')\n",
        "swords.append(\"the\")\n",
        "swords.append(\"we\")\n",
        "swords.append(\"this\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read in training data as pdfs\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "fileNames = []\n",
        "numPages = []\n",
        "dirs = [\"/content/2017/\",\"/content/2018/\",\"/content/2019/\",\"/content/2020/\"]\n",
        "\n",
        "for ss in dirs:\n",
        "\n",
        "  for filename in os.listdir(ss):\n",
        "     \n",
        "          dir = ss+ filename\n",
        "          pdffileobj=open(dir,'rb')\n",
        "          pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
        "          pageNums = pdfreader.numPages\n",
        "          fileNames.append(filename[:-4])\n",
        "          numPages.append(pageNums)\n",
        "\n",
        "zipped = list(zip(fileNames, numPages))\n",
        "featDF = pd.DataFrame(zipped, columns=['Name', 'numPages'])"
      ],
      "metadata": {
        "id": "Uk0-RCq-I58A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#marks csv\n",
        "marks = pd.read_csv('/content/docs/ml4ed_marks.2022-08-08.csv')"
      ],
      "metadata": {
        "id": "ZcoUyBvnI9C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getMark(markDF,reportID):\n",
        "  index = markDF.loc[markDF['report_id'] == reportID]\n",
        "  index = index.mark\n",
        "  return index.item()\n",
        "\n",
        "#append Student Marks\n",
        "sortedMarks = []\n",
        "for row in featDF.iterrows():\n",
        "  pdfName = row[1].Name\n",
        "  pdfName = int(pdfName)\n",
        "  m = getMark(marks,pdfName)\n",
        "  sortedMarks.append(m)\n",
        "featDF['Mark'] = sortedMarks"
      ],
      "metadata": {
        "id": "1TrslUD-JAzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read in validation data\n",
        "valFileNames = pd.read_csv('/content/docs/split_val.2022-08-08.txt',header = None)\n",
        "valFileNames.columns = [\"fileName\"]\n",
        "valFileNames = valFileNames.fileName.tolist()\n",
        "valMarks = []\n",
        "valFiles = []\n",
        "valPageNums = []\n",
        "for filename in os.listdir('/content/2021/'):\n",
        "  if int(filename[:-4]) in valFileNames:\n",
        "    dir = \"\"\n",
        "    m = getMark(marks,int(filename[:-4]))\n",
        "    valFiles.append(filename[:-4])\n",
        "    valMarks.append(m)\n",
        "    dir = '/content/2021/' + filename\n",
        "    pdffileobj=open(dir,'rb')\n",
        "    pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
        "    valPageNums.append(pdfreader.numPages)\n",
        "\n",
        "zipped = list(zip(valFiles, valMarks,valPageNums))\n",
        "val_feat_df = pd.DataFrame(zipped, columns=['Name', 'Mark',\"numPages\"])\n",
        "\n",
        "#read in test data\n",
        "valFileNames = pd.read_csv('/content/docs/split_val.2022-08-08.txt',header = None)\n",
        "valFileNames.columns = [\"fileName\"]\n",
        "valFileNames = valFileNames.fileName.tolist()\n",
        "valMarks = []\n",
        "valFiles = []\n",
        "valPageNums = []\n",
        "for filename in os.listdir('/content/2021/'):\n",
        "  if int(filename[:-4]) not in valFileNames:\n",
        "    dir = \"\"\n",
        "    m = getMark(marks,int(filename[:-4]))\n",
        "    valFiles.append(filename[:-4])\n",
        "    valMarks.append(m)\n",
        "    dir = '/content/2021/' + filename\n",
        "    pdffileobj=open(dir,'rb')\n",
        "    pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
        "    valPageNums.append(pdfreader.numPages)\n",
        "\n",
        "zipped = list(zip(valFiles, valMarks,valPageNums))\n",
        "test_feat_df = pd.DataFrame(zipped, columns=['Name', 'Mark',\"numPages\"])"
      ],
      "metadata": {
        "id": "oTwdp754JEZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Features"
      ],
      "metadata": {
        "id": "gYsMGbb_JSvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip text only files\n",
        "!unzip -qq /content/20170005_00001.zip\n",
        "!unzip -qq /content/20180028_00001.zip\n",
        "!unzip -qq /content/20190015_00001.zip\n",
        "!unzip -qq /content/20200006_00001.zip\n",
        "!unzip -qq /content/20210019_00001.zip"
      ],
      "metadata": {
        "id": "j2wWSBboJf7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getPageNums_and_Mark(DF, reportID):\n",
        "  index = DF.loc[DF['Name'] == reportID]\n",
        "  m = index.Mark\n",
        "  p = index.numPages\n",
        "  return m.item(), p.item()\n",
        "\n",
        "def get_txt_feats(txt_fn):\n",
        "    with open(txt_fn) as f:\n",
        "        lines = f.readlines()\n",
        "        # text = \n",
        "\n",
        "    n_words = 0\n",
        "    spell_mistakes = []\n",
        "    for word in wordpunct_tokenize(\"\".join(lines)):\n",
        "        if not word.isalpha():\n",
        "            continue\n",
        "        n_words += 1\n",
        "        if not (word.lower() in en_words):\n",
        "            spell_mistakes.append(word)\n",
        "\n",
        "    n_figures = 0\n",
        "    n_tables = 0\n",
        "    n_equations = 0\n",
        "    n_references = 0\n",
        "    n_lines_start_at = 0\n",
        "    for line in lines:\n",
        "        # if re.match(\"^figure.*\", line.lower()):\n",
        "        if re.match(r\"^figure.*[^\\d]$\", line.strip().lower()):\n",
        "            # print(\"Fig:\", line)\n",
        "            n_figures += 1\n",
        "        # elif re.match(\"^table.*\", line.strip().lower()):\n",
        "        elif re.match(\"^table.*[^\\d]$\", line.strip().lower()):\n",
        "            # print(\"Tab:\", line)\n",
        "            n_tables += 1\n",
        "        elif re.match(\"^\\([\\d\\.]*\\)$\", line.strip().lower()):\n",
        "            # print(\"Eq:\", line)\n",
        "            n_equations += 1\n",
        "        elif re.match(r\"^\\[\\d*\\]\", line.lower()):\n",
        "            n_references += 1\n",
        "        elif re.match(r\"^\\@.*\", line.lower()):\n",
        "            n_lines_start_at += 1\n",
        "\n",
        "    return {\n",
        "        \"n_words\": n_words,\n",
        "        \"spell_mistakes\": spell_mistakes,\n",
        "        \"n_figures\": n_figures,\n",
        "        \"n_tables\": n_tables,\n",
        "        \"n_equations\": n_equations,\n",
        "        \"n_references\": n_references,\n",
        "        \"n_lines_start_at\": n_lines_start_at\n",
        "        }"
      ],
      "metadata": {
        "id": "zvV4L8nzJWuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64m08R4uQfbO"
      },
      "outputs": [],
      "source": [
        "# extract training data features\n",
        "n_words_List = []\n",
        "spell_mistakes_List = []\n",
        "spell_mistake_types_list = []\n",
        "figure_list = []\n",
        "tables_list = []\n",
        "equation_list = []\n",
        "n_ref_list = []\n",
        "n_lines_start_list = []\n",
        "projectNames = []\n",
        "projectMarks = []\n",
        "pageCounts = []\n",
        "\n",
        "for filename in os.listdir('/content/HERMAN_TEXT'):\n",
        "  if filename[0] != \"s\":\n",
        "    if filename.endswith('txt') and int(filename[:4]) != 2021:\n",
        "      \n",
        "      name = '/content/HERMAN_TEXT/'+ filename\n",
        "      # n_words, spell_mistakes, n_figures, n_tables, n_equations, n_references, n_lines_start_at = get_txt_feats(filename)\n",
        "      features = get_txt_feats(name)\n",
        "      n_words_List.append(features['n_words'])\n",
        "      spell_mistakes_List.append(len(features[\"spell_mistakes\"]))\n",
        "      spell_mistake_types_list.append(len(set(features[\"spell_mistakes\"])))\n",
        "      figure_list.append(features[\"n_figures\"])\n",
        "      tables_list.append(features[\"n_tables\"])\n",
        "      equation_list.append(features[\"n_equations\"])\n",
        "      n_ref_list.append(features[\"n_references\"])\n",
        "      n_lines_start_list.append(features[\"n_lines_start_at\"])\n",
        "      projectNames.append(filename[:8])\n",
        "      p_mark, p_count = getPageNums_and_Mark(featDF, filename[:8])\n",
        "      projectMarks.append(p_mark)\n",
        "      pageCounts.append(p_count)\n",
        "    \n",
        "zipped = list(zip(projectNames,n_words_List, pageCounts, spell_mistakes_List,spell_mistake_types_list,figure_list,tables_list,equation_list,n_ref_list, n_lines_start_list,projectMarks))\n",
        "feature_train_df = pd.DataFrame(zipped, columns=['reportID', 'numWords',\"numPages\",\"n_spell_mistakes\",'n_spell_mistakes_types',\"n_figures\",\"n_tables\",\"n_equations\",\"n_references\",\"n_lines_start\",\"mark\"])\n",
        "\n",
        "# extract features from validation data\n",
        "\n",
        "n_words_List = []\n",
        "spell_mistakes_List = []\n",
        "spell_mistake_types_list = []\n",
        "figure_list = []\n",
        "tables_list = []\n",
        "equation_list = []\n",
        "n_ref_list = []\n",
        "n_lines_start_list = []\n",
        "projectNames = []\n",
        "projectMarks = []\n",
        "pageCounts = []\n",
        "\n",
        "for filename in os.listdir('/content/HERMAN_TEXT'):\n",
        "  if filename[0] != \"s\":\n",
        "    if filename.endswith('txt') and int(filename[:4]) == 2021 and int(filename[5:8])<=59:\n",
        "\n",
        "      name = '/content/HERMAN_TEXT/'+ filename\n",
        "      # n_words, spell_mistakes, n_figures, n_tables, n_equations, n_references, n_lines_start_at = get_txt_feats(filename)\n",
        "      features = get_txt_feats(name)\n",
        "      n_words_List.append(features['n_words'])\n",
        "      spell_mistakes_List.append(len(features[\"spell_mistakes\"]))\n",
        "      spell_mistake_types_list.append(len(set(features[\"spell_mistakes\"])))\n",
        "      figure_list.append(features[\"n_figures\"])\n",
        "      tables_list.append(features[\"n_tables\"])\n",
        "      equation_list.append(features[\"n_equations\"])\n",
        "      n_ref_list.append(features[\"n_references\"])\n",
        "      n_lines_start_list.append(features[\"n_lines_start_at\"])\n",
        "      projectNames.append(filename[:8])\n",
        "      p_mark, p_count = getPageNums_and_Mark(val_feat_df, filename[:8])\n",
        "      projectMarks.append(p_mark)\n",
        "      pageCounts.append(p_count)\n",
        "\n",
        "\n",
        "zipped = list(zip(projectNames,n_words_List, pageCounts, spell_mistakes_List,spell_mistake_types_list,figure_list,tables_list,equation_list,n_ref_list, n_lines_start_list,projectMarks))\n",
        "feature_val_df = pd.DataFrame(zipped, columns=['reportID', 'numWords',\"numPages\",\"n_spell_mistakes\",'n_spell_mistakes_types',\"n_figures\",\"n_tables\",\"n_equations\",\"n_references\",\"n_lines_start\",\"mark\"])\n",
        "\n",
        "# extract features from test data\n",
        "\n",
        "n_words_List = []\n",
        "spell_mistakes_List = []\n",
        "spell_mistake_types_list = []\n",
        "figure_list = []\n",
        "tables_list = []\n",
        "equation_list = []\n",
        "n_ref_list = []\n",
        "n_lines_start_list = []\n",
        "projectNames = []\n",
        "projectMarks = []\n",
        "pageCounts = []\n",
        "\n",
        "for filename in os.listdir('/content/HERMAN_TEXT'):\n",
        "  if filename[0] != \"s\":\n",
        "    if filename.endswith('txt') and int(filename[:4]) == 2021 and int(filename[5:8])>59:\n",
        "\n",
        "      name = '/content/HERMAN_TEXT/'+ filename\n",
        "      # n_words, spell_mistakes, n_figures, n_tables, n_equations, n_references, n_lines_start_at = get_txt_feats(filename)\n",
        "      features = get_txt_feats(name)\n",
        "      n_words_List.append(features['n_words'])\n",
        "      spell_mistakes_List.append(len(features[\"spell_mistakes\"]))\n",
        "      spell_mistake_types_list.append(len(set(features[\"spell_mistakes\"])))\n",
        "      figure_list.append(features[\"n_figures\"])\n",
        "      tables_list.append(features[\"n_tables\"])\n",
        "      equation_list.append(features[\"n_equations\"])\n",
        "      n_ref_list.append(features[\"n_references\"])\n",
        "      n_lines_start_list.append(features[\"n_lines_start_at\"])\n",
        "      projectNames.append(filename[:8])\n",
        "      p_mark, p_count = getPageNums_and_Mark(test_feat_df, filename[:8])\n",
        "      projectMarks.append(p_mark)\n",
        "      pageCounts.append(p_count)\n",
        "      \n",
        "zipped = list(zip(projectNames,n_words_List, pageCounts, spell_mistakes_List,spell_mistake_types_list,figure_list,tables_list,equation_list,n_ref_list, n_lines_start_list,projectMarks))\n",
        "feature_test_df = pd.DataFrame(zipped, columns=['reportID', 'numWords',\"numPages\",\"n_spell_mistakes\",'n_spell_mistakes_types',\"n_figures\",\"n_tables\",\"n_equations\",\"n_references\",\"n_lines_start\",\"mark\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File size"
      ],
      "metadata": {
        "id": "UcXXgzgNKayG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileSize = []\n",
        "fileMark = []\n",
        "trainNames = []\n",
        "dirs = [\"/content/2017/\",\"/content/2018/\",\"/content/2019/\",\"/content/2020/\"]\n",
        "\n",
        "for ss in dirs:\n",
        "\n",
        "  for filename in os.listdir(ss):\n",
        "     \n",
        "          dir = ss+ filename\n",
        "          filename = dir\n",
        "          file_stats = os.stat(filename)\n",
        "          fileSize.append(file_stats.st_size / (1024 * 1024))\n",
        "          m = getMark(marks,int(filename[14:-4]))\n",
        "          fileMark.append(m)\n",
        "          trainNames.append(int(filename[14:-4]))\n",
        "\n",
        "\n",
        "valFileNames = pd.read_csv('/content/docs/split_val.2022-08-08.txt',header = None)\n",
        "valFileNames.columns = [\"fileName\"]\n",
        "valFileNames = valFileNames.fileName.tolist()\n",
        "valMarks = []\n",
        "valFileSize = []\n",
        "valNames = []\n",
        "\n",
        "for filename in os.listdir('/content/2021/'):\n",
        "  if int(filename[:-4]) in valFileNames:\n",
        "    dir = \"\"\n",
        "    m = getMark(marks,int(filename[:-4]))\n",
        "    valMarks.append(m)\n",
        "    dir = '/content/2021/' + filename\n",
        "    filename = dir\n",
        "    file_stats = os.stat(filename)\n",
        "    valFileSize.append(file_stats.st_size / (1024 * 1024))\n",
        "    valNames.append(int(filename[14:-4]))\n"
      ],
      "metadata": {
        "id": "Jlennv-h97Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valFileNames = pd.read_csv('/content/docs/split_val.2022-08-08.txt',header = None)\n",
        "valFileNames.columns = [\"fileName\"]\n",
        "valFileNames = valFileNames.fileName.tolist()\n",
        "valMarks = []\n",
        "valFileSize = []\n",
        "valNames = []\n",
        "\n",
        "for filename in os.listdir('/content/2021/'):\n",
        "  if int(filename[:-4]) not in valFileNames:\n",
        "    dir = \"\"\n",
        "    m = getMark(marks,int(filename[:-4]))\n",
        "    valMarks.append(m)\n",
        "    dir = '/content/2021/' + filename\n",
        "    filename = dir\n",
        "    file_stats = os.stat(filename)\n",
        "    valFileSize.append(file_stats.st_size / (1024 * 1024))\n",
        "    valNames.append(int(filename[14:-4]))\n",
        "\n",
        "zipped = list(zip(valNames, valFileSize))\n",
        "featDF = pd.DataFrame(zipped, columns=['Name', 'fileSize'])\n",
        "featDF.to_csv(\"test_FILESIZE.csv\")"
      ],
      "metadata": {
        "id": "xnkCMyg_B2QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipped = list(zip(trainNames, fileSize))\n",
        "featDF = pd.DataFrame(zipped, columns=['Name', 'fileSize'])\n",
        "featDF.to_csv(\"train_FILESIZE.csv\")\n",
        "\n",
        "zipped = list(zip(valNames, valFileSize))\n",
        "featDF = pd.DataFrame(zipped, columns=['Name', 'fileSize'])\n",
        "featDF.to_csv(\"val_FILESIZE.csv\")"
      ],
      "metadata": {
        "id": "sUEjhOhf_wAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Count"
      ],
      "metadata": {
        "id": "SwYwE-4TKmhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "analysys_tokens = []\n",
        "trainText = []\n",
        "trainLabels = []\n",
        "trainMarks = []\n",
        "\n",
        "#text files directory\n",
        "for filename in os.listdir('/content/HERMAN_TEXT'):\n",
        "  if filename[0] != \"s\":\n",
        "    if filename.endswith('txt') and int(filename[:4]) != 2021 and int(filename[:8])!=20170049:\n",
        "      \n",
        "      name = '/content/HERMAN_TEXT/'+ filename\n",
        "      with open(name) as f:\n",
        "        lines = f.read()\n",
        "\n",
        "      lines = lines.lower()\n",
        "      text_tokens = word_tokenize(lines)\n",
        "      tokens_without_sw = [word for word in text_tokens if not word in swords]\n",
        "      analysys_tokens.append(tokens_without_sw)\n",
        "      text = (\" \").join(tokens_without_sw)\n",
        "      trainText.append(text)\n",
        "      trainLabels.append(filename[:8])\n",
        "      trainMarks.append(getMark(df_concat,int(filename[:8])))\n",
        "      \n",
        "\n",
        "      \n",
        "valText = []\n",
        "valLabels = []  \n",
        "valMarks = []\n",
        "\n",
        "for filename in os.listdir('/content/HERMAN_TEXT'):\n",
        "  if filename[0] != \"s\":\n",
        "    if filename.endswith('txt') and int(filename[:4]) == 2021 and int(filename[5:8])<=59:\n",
        "\n",
        "      name = '/content/HERMAN_TEXT/'+ filename\n",
        "\n",
        "      with open(name) as f:\n",
        "        lines = f.read()\n",
        "\n",
        "      lines = lines.lower()\n",
        "      text_tokens = word_tokenize(lines)\n",
        "      tokens_without_sw = [word for word in text_tokens if not word in swords]\n",
        "      text = (\" \").join(tokens_without_sw)\n",
        "      valText.append(text)\n",
        "      valLabels.append(filename[:8])\n",
        "      valMarks.append(getMark(df_concat,int(filename[:8])))\n",
        "\n",
        "testText = []\n",
        "testLabels = []  \n",
        "\n",
        "for filename in os.listdir('/content/HERMAN_TEXT'):\n",
        "  if filename[0] != \"s\":\n",
        "    if filename.endswith('txt') and int(filename[:4]) == 2021 and int(filename[5:8]) > 59:\n",
        "\n",
        "      name = '/content/HERMAN_TEXT/'+ filename\n",
        "\n",
        "      with open(name) as f:\n",
        "        lines = f.read()\n",
        "\n",
        "      lines = lines.lower()\n",
        "      text_tokens = word_tokenize(lines)\n",
        "      tokens_without_sw = [word for word in text_tokens if not word in swords]\n",
        "      text = (\" \").join(tokens_without_sw)\n",
        "      testText.append(text)\n",
        "      testLabels.append(filename[:8])"
      ],
      "metadata": {
        "id": "9NYB6SsWKnxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess\n",
        "\n",
        "def preProcess(training_sentences):\n",
        "  \n",
        "  for i in range(len(training_sentences)):\n",
        "    training_sentences[i] = training_sentences[i].lower()\n",
        "    training_sentences[i] = re.sub(\"-\", '', training_sentences[i])\n",
        "    training_sentences[i] = re.sub(\",\", '', training_sentences[i])\n",
        "    training_sentences[i] = re.sub(r\"\\(\", '', training_sentences[i])\n",
        "    training_sentences[i] = re.sub(r\"\\)\", '', training_sentences[i])\n",
        "    training_sentences[i] = re.sub(\";\", '', training_sentences[i])\n",
        "    training_sentences[i] = re.sub(\":\", '', training_sentences[i])\n",
        "    training_sentences[i] = re.sub(r'\\\\', ' ', training_sentences[i])\n",
        "    training_sentences[i] = re.sub(r\"[^a-z\\s]\", '', training_sentences[i])\n",
        "\n",
        "  return training_sentences"
      ],
      "metadata": {
        "id": "RjRQzqNzLjt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainText = preProcess(trainText)\n",
        "valText = preProcess(valText)\n",
        "testText = preProcess(testText)"
      ],
      "metadata": {
        "id": "sFyZOE6cLk5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract vocubulary\n",
        "def extract_words(data):\n",
        "  V = {}\n",
        "  \n",
        "  for i in range(len(data)):\n",
        "    words = data[i].split()\n",
        "    \n",
        "    for j in words:\n",
        "      if j in V:\n",
        "        V[j] +=1\n",
        "      else:\n",
        "        V[j] = 1\n",
        "\n",
        "  return V\n",
        "\n",
        "def sortFreqDict(freqdict):\n",
        "    aux = [(freqdict[key], key) for key in freqdict]\n",
        "    aux.sort()\n",
        "    aux.reverse()\n",
        "    return aux"
      ],
      "metadata": {
        "id": "TB_m5RVxLlt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_train = extract_words(trainText)\n",
        "V_train_sorted = sortFreqDict(V_train)"
      ],
      "metadata": {
        "id": "l-3pxceMLm0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_words = 10\n",
        "count_words = []\n",
        "for k in range(top_k_words):\n",
        "  # if len(V_train_sorted[k][1]) >1:\n",
        "    count_words.append(V_train_sorted[k][1])"
      ],
      "metadata": {
        "id": "eu6qOg1LLqiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def countWords(top_k_words, text):\n",
        "  wCounts = []\n",
        "  for j in range(len(top_k_words)):\n",
        "    count = 0\n",
        "    \n",
        "    words = text.split()\n",
        "\n",
        "    for k in words:\n",
        "      if k == top_k_words[j]:\n",
        "        count += 1\n",
        "\n",
        "    wCounts.append(count)\n",
        "  return wCounts"
      ],
      "metadata": {
        "id": "o9_j6dn8Lr5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts_train = []\n",
        "\n",
        "for i in range(len(trainText)):\n",
        "\n",
        "  top_counts = countWords(count_words, trainText[i])\n",
        "  counts_train.append(top_counts)\n",
        "\n",
        "counts_val = []\n",
        "\n",
        "for i in range(len(valText)):\n",
        "\n",
        "  top_counts = countWords(count_words, valText[i])\n",
        "  counts_val.append(top_counts)\n",
        "\n",
        "\n",
        "counts_test = []\n",
        "\n",
        "for i in range(len(testText)):\n",
        "\n",
        "  top_counts = countWords(count_words, testText[i])\n",
        "  counts_test.append(top_counts)"
      ],
      "metadata": {
        "id": "YyMDigXNLs4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = []\n",
        "for i in range(10):\n",
        "  name = \"count_\" + str(i)\n",
        "  col_names.append(name)"
      ],
      "metadata": {
        "id": "-hg5UbDbLuBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipped = list(zip(trainLabels, counts_train))\n",
        "featDF_train = pd.DataFrame(zipped, columns=['report_id', 'counts'])\n",
        "m = pd.DataFrame(featDF_train[\"counts\"].to_list(), columns=col_names)\n",
        "featDF_train = pd.concat([featDF_train[\"report_id\"],m],axis = 1)\n",
        "\n",
        "\n",
        "zipped = list(zip(valLabels, counts_val))\n",
        "featDF_val = pd.DataFrame(zipped, columns=['report_id', 'counts'])\n",
        "m = pd.DataFrame(featDF_val[\"counts\"].to_list(), columns=col_names)\n",
        "featDF_val = pd.concat([featDF_val[\"report_id\"],m],axis = 1)\n",
        "\n",
        "zipped = list(zip(testLabels, counts_val))\n",
        "featDF_test = pd.DataFrame(zipped, columns=['report_id', 'counts'])\n",
        "m = pd.DataFrame(featDF_test[\"counts\"].to_list(), columns=col_names)\n",
        "featDF_test = pd.concat([featDF_test[\"report_id\"],m],axis = 1)"
      ],
      "metadata": {
        "id": "DU4m1J-LLvvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTOPIC"
      ],
      "metadata": {
        "id": "8hD-OLScLxm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "swords = stopwords.words('english')\n",
        "swords.append(\"the\")\n",
        "swords.append(\"we\")\n",
        "swords.append(\"this\")\n",
        "swords.append(\"project\")\n",
        "swords.append(\"report\")\n",
        "swords.append(\"using\")"
      ],
      "metadata": {
        "id": "f6RzIDwoLytE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ABSTRACT ONLY TEXT\n",
        "AbstractText = []\n",
        "id_names = []\n",
        "years = []\n",
        "#TRAIN\n",
        "for filename in os.listdir(\"/content/EEabstracts/\"):\n",
        "\n",
        "  file = '/content/EEabstracts/' + filename\n",
        "  if filename[:4] != \"2021\":\n",
        "    with open(file) as f:\n",
        "          lines = f.read()\n",
        "    f.close()\n",
        "    \n",
        "    lines= lines.lower()\n",
        "    text_tokens = word_tokenize(lines)\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in swords]\n",
        "    text = (\" \").join(tokens_without_sw)\n",
        "    years.append(filename[:4])\n",
        "    AbstractText.append(text)\n",
        "    id_names.append(filename[:8])\n",
        "\n",
        "\n",
        "zipped = list(zip(id_names,AbstractText, years))\n",
        "summaryDF = pd.DataFrame(zipped, columns=['reportID', \"sumText\",\"year\"])\n",
        "\n",
        "#VALIDATION\n",
        "AbstractText = []\n",
        "id_names = []\n",
        "\n",
        "for filename in os.listdir(\"/content/EEabstracts/\"):\n",
        "\n",
        "  file = '/content/EEabstracts/' + filename\n",
        "  if int(filename[:8]) <= 20210059 and filename[:4] == \"2021\":\n",
        "    with open(file) as f:\n",
        "          lines = f.read()\n",
        "    f.close()\n",
        "    lines = lines.lower()\n",
        "    text_tokens = word_tokenize(lines)\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in swords]\n",
        "    text = (\" \").join(tokens_without_sw)\n",
        "\n",
        "    AbstractText.append(text)\n",
        "    id_names.append(filename[:8])\n",
        "\n",
        "zipped = list(zip(id_names,AbstractText))\n",
        "summaryDFVAL = pd.DataFrame(zipped, columns=['reportID', \"sumText\"])\n",
        "\n",
        "#TEST\n",
        "AbstractText = []\n",
        "id_names = []\n",
        "\n",
        "for filename in os.listdir(\"/content/EEabstracts/\"):\n",
        "\n",
        "  file = '/content/EEabstracts/' + filename\n",
        "  if int(filename[:8]) > 20210059 and filename[:4] == \"2021\":\n",
        "    with open(file) as f:\n",
        "          lines = f.read()\n",
        "    f.close()\n",
        "    lines = lines.lower()\n",
        "    text_tokens = word_tokenize(lines)\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in swords]\n",
        "    text = (\" \").join(tokens_without_sw)\n",
        "\n",
        "    AbstractText.append(text)\n",
        "    id_names.append(filename[:8])\n",
        "\n",
        "zipped = list(zip(id_names,AbstractText))\n",
        "summaryDFTEST = pd.DataFrame(zipped, columns=['reportID', \"sumText\"])"
      ],
      "metadata": {
        "id": "gs3upNKcL_v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq bertopic\n",
        "!pip install -qq bertopic[visualization]\n",
        "from bertopic import BERTopic\n",
        "!pip install -U kaleido\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "oY9fgUHdMOpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cluster_model = KMeans(n_clusters = 8)\n",
        "model = BERTopic(verbose=True,nr_topics = 8,language = \"English\",hdbscan_model = cluster_model)\n",
        " \n",
        "#convert to list \n",
        "docs = summaryDF.sumText.to_list()\n",
        "dates = summaryDF.year.to_list()\n",
        " \n",
        "topics, probabilities = model.fit_transform(docs)"
      ],
      "metadata": {
        "id": "ufZFXX_pMTPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_over_time = model.topics_over_time(docs, dates)\n",
        "c = model.visualize_topics_over_time(topics_over_time)\n",
        "# c.update_layout(title = \"\")\n",
        "c.update_xaxes(nticks = 4)\n",
        "c.write_image(\"/content/topicsOverTime.pdf\")\n",
        "c.show()"
      ],
      "metadata": {
        "id": "mrs0FKq4MYqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = model.generate_topic_labels(nr_words=3, topic_prefix=True, word_length=None, separator='_')"
      ],
      "metadata": {
        "id": "4bnXWUOZMZ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels"
      ],
      "metadata": {
        "id": "j4jxm_MDMapO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart(n_words = 5)"
      ],
      "metadata": {
        "id": "rwyCU8n0McIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get val data topics\n",
        "valTopics = []\n",
        "\n",
        "for i in summaryDFVAL.sumText:\n",
        "  new_doc = i\n",
        "  t = model.transform([new_doc])\n",
        "  valTopics.append(t[0][0])\n",
        "\n",
        "#get test data topics\n",
        "testTopics = []\n",
        "\n",
        "for i in summaryDFTEST.sumText:\n",
        "  new_doc = i\n",
        "  t = model.transform([new_doc])\n",
        "  testTopics.append(t[0][0])\n",
        "\n",
        "summaryDF[\"topic\"] = topics"
      ],
      "metadata": {
        "id": "T2Vr2Z5bMdTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = summaryDF.reportID.tolist()\n",
        "id_marks = []\n",
        "for i in range(len(ids)):\n",
        "\n",
        "  iv_mark = getMark(marks,int(ids[i]))\n",
        "  id_marks.append(iv_mark)\n",
        "\n",
        "summaryDF[\"mark\"] = id_marks\n",
        "\n",
        "ids = summaryDFVAL.reportID.tolist()\n",
        "id_marks = []\n",
        "for i in range(len(ids)):\n",
        "\n",
        "  iv_mark = getMark(marks,int(ids[i]))\n",
        "  id_marks.append(iv_mark)\n",
        "\n",
        "summaryDFVAL[\"mark\"] = id_marks\n",
        "\n",
        "ids = summaryDFTEST.reportID.tolist()\n",
        "id_marks = []\n",
        "for i in range(len(ids)):\n",
        "\n",
        "  iv_mark = getMark(marks,int(ids[i]))\n",
        "  id_marks.append(iv_mark)\n",
        "\n",
        "summaryDFTEST[\"mark\"] = id_marks"
      ],
      "metadata": {
        "id": "QSuFiXYQMgoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def returnTopic(df,reportID):\n",
        "  index = df[df['reportID'] == reportID].topic.tolist()\n",
        "  index = index[0] \n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "xo6Tt1OsMksH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}